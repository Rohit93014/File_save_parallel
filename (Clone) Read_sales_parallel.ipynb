{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c508a9-8960-4f98-9cd7-1267b3594e76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# read the table\n",
    "df = spark.table(\"sales_file_catalog.default.sales_20_days_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47125238-d913-415b-bf12-b5947099489c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c36a30cf-5a60-49d9-83a5-18fd0c5004e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from datetime import datetime\n",
    "\n",
    "# Load source table\n",
    "df = spark.table(\"sales_file_catalog.default.sales_20_days_clean\")\n",
    "\n",
    "# Collect unique dates\n",
    "unique_dates = df.select(\"date\").distinct().collect()\n",
    "\n",
    "# Base output path\n",
    "base_path = \"/Volumes/sales_file_catalog/default/output_sales/sales_data\"\n",
    "\n",
    "# Loop through each unique date\n",
    "for row in unique_dates:\n",
    "    raw_date = row[\"date\"]  # could be string or DateType\n",
    "\n",
    "    # Ensure raw_date is a datetime object\n",
    "    if isinstance(raw_date, str):\n",
    "        raw_date = datetime.strptime(raw_date, \"%d-%m-%Y\")\n",
    "\n",
    "    year = raw_date.strftime(\"%Y\")\n",
    "    month = raw_date.strftime(\"%m\")\n",
    "    day = raw_date.strftime(\"%d\")\n",
    "\n",
    "    # Construct directory path\n",
    "    output_path = f\"{base_path}/{year}/{month}/{day}\"\n",
    "\n",
    "    # Filter data\n",
    "    filtered_df = df.filter(col(\"date\") == row[\"date\"])\n",
    "\n",
    "    # # Save as Delta file\n",
    "    # filtered_df.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "    filtered_df.write.format(\"csv\").mode(\"overwrite\").save(output_path + '_csv')\n",
    "\n",
    "    print(f\" Saved Delta files to:{output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "708ae4a8-7874-4f0b-ad12-ccaf91c9417a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "from datetime import datetime\n",
    "\n",
    "# Step 1: Read the table\n",
    "df = spark.table(\"sales_file_catalog.default.sales_20_days_clean\")\n",
    "\n",
    "# Step 2: Unique dates\n",
    "unique_dates = df.select(\"date\").distinct().collect()\n",
    "\n",
    "# Step 3: Base path\n",
    "base_path = \"/Volumes/sales_file_catalog/default/output_sales/sales_data\"\n",
    "\n",
    "# Step 4: Loop through each date\n",
    "for row in unique_dates:\n",
    "    date_value = row[\"date\"]\n",
    "    \n",
    "    # Convert to datetime object\n",
    "    date_obj = datetime.strptime(str(date_value), \"%Y-%m-%d\")\n",
    "\n",
    "    # Extract year, month, day\n",
    "    year = date_obj.strftime(\"%Y\")\n",
    "    month = date_obj.strftime(\"%m\")\n",
    "    day = date_obj.strftime(\"%d\")\n",
    "\n",
    "    # Path to save\n",
    "    output_path = f\"{base_path}/{year}/{month}/{day}\"\n",
    "\n",
    "    # Filter for this date\n",
    "    df_filtered = df.filter(col(\"date\") == date_value)\n",
    "\n",
    "    # Save in Delta format\n",
    "    df_filtered.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "\n",
    "    print(f\" Saved: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae33ba8-99d3-4c5a-9812-c29db9f261b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Import required libraries\n",
    "from pyspark.sql.functions import col\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "#Initialize log list\n",
    "log_data = []\n",
    "\n",
    "#Load source table\n",
    "df = spark.table(\"sales_file_catalog.default.sales_20_days_clean\")\n",
    "\n",
    "#Get distinct dates from the dataset\n",
    "unique_dates = df.select(\"date\").distinct().collect()\n",
    "\n",
    "#Set base output path\n",
    "base_path = \"/Volumes/sales_file_catalog/default/output_sales/sales_data\"\n",
    "\n",
    "#  Loop through each date and save data partition-wise\n",
    "for row in unique_dates:\n",
    "    date_value = row[\"date\"]\n",
    "    \n",
    "    #Initialize log entry with date\n",
    "    log_entry = {\"date\": str(date_value)}\n",
    "    \n",
    "    try:\n",
    "        #Start timing\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Parse date and build folder path\n",
    "        date_obj = datetime.strptime(str(date_value), \"%Y-%m-%d\")\n",
    "        year = date_obj.strftime(\"%Y\")\n",
    "        month = date_obj.strftime(\"%m\")\n",
    "        day = date_obj.strftime(\"%d\")\n",
    "        output_path = f\"{base_path}/{year}/{month}/{day}\"\n",
    "\n",
    "        # Filter data for this date\n",
    "        df_filtered = df.filter(col(\"date\") == date_value)\n",
    "\n",
    "        #Count records\n",
    "        record_count = df_filtered.count()\n",
    "\n",
    "        #Save to Delta\n",
    "        df_filtered.write.format(\"delta\").mode(\"overwrite\").save(output_path)\n",
    "\n",
    "        #Log duration and status\n",
    "        duration = round(time.time() - start_time, 2)\n",
    "        log_entry.update({\n",
    "            \"status\": \"SUCCESS\",\n",
    "            \"output_path\": output_path,\n",
    "            \"records\": record_count,\n",
    "            \"duration_sec\": duration\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        #Log error info\n",
    "        log_entry.update({\n",
    "            \"status\": \"ERROR\",\n",
    "            \"output_path\": None,\n",
    "            \"records\": None,\n",
    "            \"duration_sec\": None,\n",
    "            \"error\": str(e)\n",
    "        })\n",
    "\n",
    "    #Append log entry to list\n",
    "    log_data.append(log_entry)\n",
    "\n",
    "#Convert log to Spark DataFrame\n",
    "log_spark_df = spark.createDataFrame(log_data)\n",
    "\n",
    "#Display logs\n",
    "display(log_spark_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6e87c4f-9ebf-49ff-96d2-0d1db972cf8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) Read_sales_parallel",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
